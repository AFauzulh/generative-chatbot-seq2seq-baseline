{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strong-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from random import seed, randrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sacrebleu\n",
    "import bert_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from models.LSTMBahdanau import Encoder, Decoder, Seq2Seq\n",
    "# from models.BiLSTMLuong import Encoder, Decoder, Seq2Seq\n",
    "from utils.tokenizer import Tokenizer, pad_sequences, respond, MyData\n",
    "from utils.tokenizer import respond_only_lstm_attn, respond_only_lstm_no_attn, respond_only_gru_no_attn, respond_only_gru_attn\n",
    "from utils.preprocess import preprocess_1, preprocess_2\n",
    "from trainer import train, loss_function, sort_within_batch\n",
    "from utils.evaluate import calculate_rouge, calculate_bertscore, calculate_bleu\n",
    "\n",
    "root_dir = '/home/alfirsafauzulh@student.ub.ac.id/Firsa/Research/Chatbot'\n",
    "\n",
    "data_dir = root_dir + '/Datasets'\n",
    "dailydialogs_root_dir = data_dir + '/dailydialog'\n",
    "cornell_root_dir = data_dir + '/cornell_movie'\n",
    "saved_model_path = '/home/alfirsafauzulh@student.ub.ac.id/Firsa/Research/Chatbot/Code/Train/saved_models/BiGRULuong-cornell'\n",
    "\n",
    "RANDOM_SEED = 1111\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compact-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "th = 50\n",
    "# df = pd.read_csv(dailydialogs_root_dir + f'/df_dailydialogs_max_{th}.csv')\n",
    "# df = pd.read_csv(f'./Datasets/dailydialog/df_dailydialogs_max_{th}.csv')\n",
    "df = pd.read_csv(f'./Datasets/cornell_movie/df_cornell_max_{th}.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "solved-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(saved_model_path + \"/tokenizer.pickle\", 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tamil-tissue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\t: 171350\n",
      "Test\t: 42838\n"
     ]
    }
   ],
   "source": [
    "max_len = th+2\n",
    "\n",
    "df['questions_preprocessed'] = df['questions'].apply(preprocess_1)\n",
    "df['answers_preprocessed'] = df['answers'].apply(preprocess_1)\n",
    "\n",
    "df['questions_preprocessed'] = df['questions'].map(lambda x: preprocess_2(x))\n",
    "df['answers_preprocessed'] = df['answers'].map(lambda x: preprocess_2(x))\n",
    "\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=.2, random_state=RANDOM_SEED)\n",
    "print(f\"Train\\t: {len(df_train)}\")\n",
    "print(f\"Test\\t: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cooked-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BiGRULuong import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sustainable-wages",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfirsafauzulh@student.ub.ac.id/miniconda3/envs/thesis_chatbot/lib/python3.9/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(45809, 768)\n",
       "    (rnn): GRU(768, 768, dropout=0.5, bidirectional=True)\n",
       "    (fc_hidden): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (fc_encoder_states): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(45809, 768)\n",
       "    (rnn): GRU(1536, 768, dropout=0.5)\n",
       "    (fc): Linear(in_features=768, out_features=45809, bias=True)\n",
       "    (W1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (W2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (V): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size_encoder = len(tokenizer.vocab)+4\n",
    "input_size_decoder = len(tokenizer.vocab)+4\n",
    "output_size = len(tokenizer.vocab)+4\n",
    "vocab_len = len(tokenizer.vocab)+4\n",
    "\n",
    "# encoder_embedding_size = pretrained_word_embedding_dimensions\n",
    "# decoder_embedding_size = pretrained_word_embedding_dimensions\n",
    "\n",
    "encoder_embedding_size = 768\n",
    "decoder_embedding_size = 768\n",
    "\n",
    "hidden_size = 768\n",
    "batch_size = 64\n",
    "num_layers = 1\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# input_tensor_test = df_test['questions_preprocessed'].values.tolist()\n",
    "# target_tensor_test = df_test['answers_preprocessed'].values.tolist()\n",
    "\n",
    "# test_data = MyData(input_tensor_test, target_tensor_test)\n",
    "# test_dataset = DataLoader(test_data, batch_size = batch_size, drop_last=True, shuffle=True)\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, \n",
    "                  num_layers, enc_dropout, pretrained_word_embedding=False, embedding_matrix=None, freeze=False).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, \n",
    "                      output_size, num_layers, dec_dropout, pretrained_word_embedding=False, embedding_matrix=None, freeze=False).to(device)\n",
    "    \n",
    "model = Seq2Seq(encoder_net, decoder_net, vocab_len=vocab_len)\n",
    "model.load_state_dict(torch.load(saved_model_path + \"/best_loss.pth\", map_location=device))\n",
    "# model.load_state_dict(torch.load(saved_model_path + \"/model.pth\", map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "native-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = df_test['questions'].values\n",
    "test_answers = df_test['answers'].values\n",
    "\n",
    "preds = []\n",
    "for x in test_questions:\n",
    "#     preds.append(respond_only_lstm_no_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "    preds.append(respond_only_gru_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "downtown-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfirsafauzulh@student.ub.ac.id/miniconda3/envs/thesis_chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/alfirsafauzulh@student.ub.ac.id/miniconda3/envs/thesis_chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/alfirsafauzulh@student.ub.ac.id/miniconda3/envs/thesis_chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1-gram': 0.045183853155937564,\n",
       " '2-gram': 0.008124208419023492,\n",
       " '3-gram': 0.0027972806307226413,\n",
       " '4-gram': 0.0010883742640694176}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bleu(preds, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "running-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 0.07786496728658676,\n",
       " 'r': -0.03902457281947136,\n",
       " 'f': 0.018437162041664124}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bertscore(preds, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-student",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_chatbot",
   "language": "python",
   "name": "thesis_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

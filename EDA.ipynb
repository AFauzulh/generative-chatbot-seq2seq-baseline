{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "growing-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats as st\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "from utils.preprocess import preprocess_1, preprocess_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "supreme-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1111\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "th = 50\n",
    "# df = pd.read_csv(f'./Datasets/dailydialog/df_dailydialogs_max_{th}.csv')\n",
    "# df = pd.read_csv(f'./Datasets/dailydialog/df_dailydialogs_factory.csv')\n",
    "df = pd.read_csv(f'./Datasets/cornell_movie/df_cornell_factory.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "intense-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = th+2\n",
    "\n",
    "# df['questions_preprocessed'] = df['questions'].apply(preprocess_1)\n",
    "# df['answers_preprocessed'] = df['answers'].apply(preprocess_1)\n",
    "\n",
    "# df['questions_preprocessed'] = df['questions'].map(lambda x: preprocess_2(x))\n",
    "# df['answers_preprocessed'] = df['answers'].map(lambda x: preprocess_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "residential-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = df['questions'].values\n",
    "answer_list = df['answers'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "piano-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum word in questions: 319\n",
      "average word in questions: 10.88\n",
      "mode word in questions: 4 count: 19654\n",
      "====================================================\n",
      "maximum word in answers: 319\n",
      "average word in answers: 10.88\n",
      "mode word in answers: 4 count: 19654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3774478/2877074879.py:22: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  print(f\"mode word in questions: {st.mode(np.array(len_per_lines_questions)).mode[0]} count: {st.mode(np.array(len_per_lines_questions)).count[0]}\")\n",
      "/tmp/ipykernel_3774478/2877074879.py:26: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  print(f\"mode word in answers: {st.mode(np.array(len_per_lines_questions)).mode[0]} count: {st.mode(np.array(len_per_lines_questions)).count[0]}\")\n"
     ]
    }
   ],
   "source": [
    "maximum_word_questions = 0\n",
    "maximum_word_answers = 0\n",
    "\n",
    "len_per_lines_questions = []\n",
    "len_per_lines_answers = []\n",
    "\n",
    "for question, answer in zip(question_list, answer_list):\n",
    "    q = question.split(' ')\n",
    "    a = answer.split(' ')\n",
    "    \n",
    "    len_per_lines_questions.append(len(q))\n",
    "    len_per_lines_answers.append(len(a))\n",
    "    \n",
    "    if maximum_word_questions < len(q):\n",
    "        maximum_word_questions = len(q)\n",
    "        \n",
    "    if maximum_word_answers < len(a):\n",
    "        maximum_word_answers = len(a)\n",
    "\n",
    "print(f\"maximum word in questions: {maximum_word_questions}\")\n",
    "print(f\"average word in questions: {np.array(len_per_lines_questions).mean():.2f}\")\n",
    "print(f\"mode word in questions: {st.mode(np.array(len_per_lines_questions)).mode[0]} count: {st.mode(np.array(len_per_lines_questions)).count[0]}\")\n",
    "print(\"====================================================\")\n",
    "print(f\"maximum word in answers: {maximum_word_questions}\")\n",
    "print(f\"average word in answers: {np.array(len_per_lines_questions).mean():.2f}\")\n",
    "print(f\"mode word in answers: {st.mode(np.array(len_per_lines_questions)).mode[0]} count: {st.mode(np.array(len_per_lines_questions)).count[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-habitat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_chatbot",
   "language": "python",
   "name": "thesis_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "protecting-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# sys.path.append(os.path.abspath(os.path.join('..', 'config')))\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from random import seed, randrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sacrebleu\n",
    "import bert_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.tokenizer import Tokenizer, pad_sequences, respond, MyData\n",
    "from utils.tokenizer import respond_only_lstm_attn, respond_only_lstm_no_attn, respond_only_gru_no_attn, respond_only_gru_attn\n",
    "from utils.preprocess import preprocess_1, preprocess_2\n",
    "from trainer import train, loss_function, sort_within_batch\n",
    "from utils.evaluate import calculate_rouge, calculate_bertscore, calculate_bleu\n",
    "\n",
    "root_dir = '/home/alfirsafauzulh@student.ub.ac.id/Firsa/Research/Chatbot'\n",
    "\n",
    "data_dir = root_dir + '/Datasets'\n",
    "dailydialogs_root_dir = data_dir + '/dailydialog'\n",
    "cornell_root_dir = data_dir + '/cornell_movie'\n",
    "\n",
    "RANDOM_SEED = 1111\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "celtic-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "likely-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data \t: 44164\n",
      "Test Data\t: 11042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "th = 15\n",
    "df = pd.read_csv(f\"{dailydialogs_root_dir}/df_dailydialogs_max_{th}.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "df['questions_preprocessed'] = df['questions'].apply(preprocess_1)\n",
    "df['answers_preprocessed'] = df['answers'].apply(preprocess_1)\n",
    "\n",
    "tokenizer = Tokenizer(pd.concat([df['questions'], df['answers']], axis=0).values, min_freq=1)\n",
    "\n",
    "max_len = th+2\n",
    "df['questions_preprocessed'] = df['questions'].map(lambda x: preprocess_2(x))\n",
    "df['answers_preprocessed'] = df['answers'].map(lambda x: preprocess_2(x))\n",
    "\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "    \n",
    "df_train, df_test = train_test_split(df, test_size=.2, random_state=RANDOM_SEED)\n",
    "print(f\"Train Data \\t: {len(df_train)}\")\n",
    "print(f\"Test Data\\t: {len(df_test)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "protecting-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30526, 768)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "custom_special_tokens = [\"<sos>\", \"<eos>\", \"<PAD>\", \"<UNK>\"]\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', return_tensors=\"pt\")\n",
    "bert_tokenizer.add_special_tokens({\"additional_special_tokens\": custom_special_tokens})\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "bert_model.resize_token_embeddings(len(bert_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "acceptable-grill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos>        30,522\n",
      "can           2,064\n",
      "you           2,017\n",
      "call          2,655\n",
      "an            2,019\n",
      "ambulance    10,771\n",
      "<eos>        30,523\n"
     ]
    }
   ],
   "source": [
    "text = df['questions'].values[333]\n",
    "marked_text = \"<sos> \" + text + \" <eos>\"\n",
    "tokenized_text = bert_tokenizer.tokenize(marked_text)\n",
    "indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "excessive-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(marked_text)\n",
    "indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "bert_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(tokens_tensor)\n",
    "    # use last hidden state as word embeddings\n",
    "    last_hidden_state = outputs[0]\n",
    "    word_embed_1 = last_hidden_state\n",
    "    \n",
    "    hidden_states = outputs[2]\n",
    "    \n",
    "    # sum of last four layer\n",
    "    word_embed = torch.stack(hidden_states[-4:]).sum(0)\n",
    "    \n",
    "    # concatenate last four layers\n",
    "    word_embed_cat = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "relevant-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "crazy-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.index2word\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "pretrained_word_embedding_dimensions = 768\n",
    "embedding_matrix = np.zeros((vocab_size, pretrained_word_embedding_dimensions))\n",
    "\n",
    "for i, word in vocab.items():\n",
    "    tokenized_text = bert_tokenizer.tokenize(word)\n",
    "    indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    \n",
    "    outputs = bert_model(tokens_tensor)\n",
    "    hidden_states = outputs[2]\n",
    "    word_embed = torch.stack(hidden_states[-4:]).sum(0).squeeze(0)\n",
    "    \n",
    "    try:\n",
    "        if word_embed is not None:\n",
    "            if word_embed.size(0) > 1 :\n",
    "                word_embed = word_embed.mean(dim=0).unsqueeze(0)\n",
    "                \n",
    "            embedding_matrix[i] = word_embed.numpy()\n",
    "        else:\n",
    "            print(\"Embedding not found\")\n",
    "    except:\n",
    "        print(f\"{word}-{word_embed.shape}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "finite-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embed(word):\n",
    "    tokenized_text = bert_tokenizer.tokenize(word)\n",
    "    indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    \n",
    "    outputs = bert_model(tokens_tensor)\n",
    "    hidden_states = outputs[2]\n",
    "    word_embed = torch.stack(hidden_states[-4:]).sum(0).squeeze(0)\n",
    "    if word_embed.size(0) > 1 :\n",
    "        word_embed = word_embed.mean(dim=0).unsqueeze(0)\n",
    "    return word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "going-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "upper-sudan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5409])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = get_word_embed(\"king\") - get_word_embed(\"man\") + get_word_embed(\"woman\")\n",
    "input2 = get_word_embed(\"queen\")\n",
    "cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "challenging-brown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6419])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = get_word_embed(\"who\")\n",
    "input2 = get_word_embed(\"whom\")\n",
    "cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-saying",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_chatbot",
   "language": "python",
   "name": "thesis_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
